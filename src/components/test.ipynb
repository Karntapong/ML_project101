{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from src.logger import logging\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DS_project\\ML_project\\src\\components\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('D:\\DS_project\\ML_project\\src\\components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\DS_project\\\\ML_project\\\\src\\\\components',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\python38.zip',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\DLLs',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\lib',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv',\n",
       " '',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\lib\\\\site-packages',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\lib\\\\site-packages\\\\ml_project-0.0.1-py3.8.egg',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\lib\\\\site-packages\\\\win32',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'd:\\\\DS_project\\\\ML_project\\\\venv\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'D:\\\\DS_project\\\\ML_project\\\\src',\n",
       " 'D:\\\\DS_project\\\\ML_project\\\\src\\\\**',\n",
       " 'D:\\\\DS_project\\\\ML_project',\n",
       " 'D:\\\\DS_project\\\\ML_project\\\\src\\\\components',\n",
       " 'D:\\\\DS_project\\\\ML_project\\\\src',\n",
       " 'D:\\\\DS_project\\\\ML_project\\\\src']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exception import CustomException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CustomException' from 'src.exception' (d:\\DS_project\\ML_project\\venv\\lib\\site-packages\\ml_project-0.0.1-py3.8.egg\\src\\exception.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_tranformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataTransformation\n",
      "File \u001b[1;32md:\\DS_project\\ML_project\\src\\components\\data_tranformation.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder,StandardScaler\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomException\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CustomException' from 'src.exception' (d:\\DS_project\\ML_project\\venv\\lib\\site-packages\\ml_project-0.0.1-py3.8.egg\\src\\exception.py)"
     ]
    }
   ],
   "source": [
    "from data_tranformation import DataTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class ModelTrainer:\n",
    "    def initiate_model_train(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Choose models and perform hyperparameter with optuna\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            model_name = trial.suggest_categorical(\"model_name\", [\"Random Forest\", \"Decision Tree\", \"Gradient Boosting\", \"Linear Regression\", \"XGBRegressor\", \"CatBoosting Regressor\", \"AdaBoost Regressor\"])\n",
    "\n",
    "            if model_name == \"Random Forest\":\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=trial.suggest_int(\"rf_n_estimators\", 8, 256),\n",
    "                    criterion='mse'  # Change 'mse' to 'mae' if you want to use Mean Absolute Error\n",
    "                )\n",
    "            elif model_name == \"Decision Tree\":\n",
    "                model = DecisionTreeRegressor(\n",
    "                    criterion='mse',\n",
    "                    splitter=trial.suggest_categorical(\"dt_splitter\", ['best', 'random'])\n",
    "                )\n",
    "            elif model_name == \"Gradient Boosting\":\n",
    "                model = GradientBoostingRegressor(\n",
    "                    learning_rate=trial.suggest_float(\"gb_learning_rate\", 0.001, 0.1),\n",
    "                    n_estimators=trial.suggest_int(\"gb_n_estimators\", 8, 256),\n",
    "                    subsample=trial.suggest_float(\"gb_subsample\", 0.6, 0.9),\n",
    "                    criterion='friedman_mse'  # Change 'friedman_mse' to 'squared_error' if needed\n",
    "                )\n",
    "            elif model_name == \"Linear Regression\":\n",
    "                model = LinearRegression()\n",
    "            elif model_name == \"XGBRegressor\":\n",
    "                model = XGBRegressor(\n",
    "                    learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.001, 0.1),\n",
    "                    n_estimators=trial.suggest_int(\"xgb_n_estimators\", 8, 256)\n",
    "                )\n",
    "            elif model_name == \"CatBoosting Regressor\":\n",
    "                model = CatBoostRegressor(\n",
    "                    depth=trial.suggest_int(\"cb_depth\", 6, 10),\n",
    "                    learning_rate=trial.suggest_float(\"cb_learning_rate\", 0.01, 0.1),\n",
    "                    iterations=trial.suggest_int(\"cb_iterations\", 30, 100),\n",
    "                    verbose=False\n",
    "                )\n",
    "            elif model_name == \"AdaBoost Regressor\":\n",
    "                model = AdaBoostRegressor(\n",
    "                    learning_rate=trial.suggest_float(\"ab_learning_rate\", 0.001, 0.1),\n",
    "                    n_estimators=trial.suggest_int(\"ab_n_estimators\", 8, 256)\n",
    "                )\n",
    "\n",
    "            # Perform cross-validation to evaluate the model\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')  # Use MSE as the scoring metric\n",
    "            score = -scores.mean()  # Take the negative mean as cross_val_score returns negative MSE\n",
    "\n",
    "            return score\n",
    "\n",
    "        # Set up the Optuna study and optimize the objective function\n",
    "        study = optuna.create_study(direction=\"minimize\")  # minimize MSE\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        # Instantiate the best model based on the Optuna results\n",
    "        best_model_name = study.best_trial.params[\"model_name\"]\n",
    "        if best_model_name == \"Random Forest\":\n",
    "            best_model = RandomForestRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"Decision Tree\":\n",
    "            best_model = DecisionTreeRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"Gradient Boosting\":\n",
    "            best_model = GradientBoostingRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"Linear Regression\":\n",
    "            best_model = LinearRegression()\n",
    "        elif best_model_name == \"XGBRegressor\":\n",
    "            best_model = XGBRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"CatBoosting Regressor\":\n",
    "            best_model = CatBoostRegressor(**study.best_trial.params, verbose=False)\n",
    "        elif best_model_name == \"AdaBoost Regressor\":\n",
    "            best_model = AdaBoostRegressor(**study.best_trial.params)\n",
    "\n",
    "        # Train the best model on the entire training set\n",
    "        best_model.fit(X_train, y_train)\n",
    "        logging.info('Find best model completed')\n",
    "        return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class ModelTrainer:\n",
    "    def initiate_model_train(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Choose models and perform hyperparameter with optuna\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            model_name = trial.suggest_categorical(\"model_name\", [\"Random Forest\", \"Decision Tree\", \"Gradient Boosting\", \"Linear Regression\", \"XGBRegressor\", \"CatBoosting Regressor\", \"AdaBoost Regressor\"])\n",
    "            if model_name == \"Random Forest\":\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=trial.suggest_int(\"rf_n_estimators\", 8, 256),\n",
    "                    criterion='squared_error'  # Modify as needed\n",
    "                )\n",
    "            elif model_name == \"Decision Tree\":\n",
    "                model = DecisionTreeRegressor(\n",
    "                    criterion='squared_error',  # Change to one of the allowed options\n",
    "                    splitter=trial.suggest_categorical(\"dt_splitter\", ['best', 'random'])\n",
    "                )\n",
    "            elif model_name == \"Gradient Boosting\":\n",
    "                model = GradientBoostingRegressor(\n",
    "                    learning_rate=trial.suggest_float(\"gb_learning_rate\", 0.001, 0.1),\n",
    "                    n_estimators=trial.suggest_int(\"gb_n_estimators\", 8, 256),\n",
    "                    subsample=trial.suggest_float(\"gb_subsample\", 0.6, 0.9),\n",
    "                    criterion='friedman_mse'  # Modify as needed\n",
    "                )\n",
    "            elif model_name == \"Linear Regression\":\n",
    "                model = LinearRegression()\n",
    "            elif model_name == \"XGBRegressor\":\n",
    "                model = XGBRegressor(\n",
    "                    learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.001, 0.1),\n",
    "                    n_estimators=trial.suggest_int(\"xgb_n_estimators\", 8, 256)\n",
    "                )\n",
    "            elif model_name == \"CatBoosting Regressor\":\n",
    "                model = CatBoostRegressor(\n",
    "                    depth=trial.suggest_int(\"cb_depth\", 6, 10),\n",
    "                    learning_rate=trial.suggest_float(\"cb_learning_rate\", 0.01, 0.1),\n",
    "                    iterations=trial.suggest_int(\"cb_iterations\", 30, 100),\n",
    "                    verbose=False\n",
    "                )\n",
    "            elif model_name == \"AdaBoost Regressor\":\n",
    "                model = AdaBoostRegressor(\n",
    "                    learning_rate=trial.suggest_float(\"ab_learning_rate\", 0.001, 0.1),\n",
    "                    n_estimators=trial.suggest_int(\"ab_n_estimators\", 8, 256)\n",
    "                )\n",
    "\n",
    "            # Perform cross-validation to evaluate the model\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')  # Use MSE as the scoring metric\n",
    "            score = -scores.mean()  # Take the negative mean as cross_val_score returns negative MSE\n",
    "\n",
    "            return score\n",
    "\n",
    "        # Set up the Optuna study and optimize the objective function\n",
    "        study = optuna.create_study(direction=\"minimize\")  # minimize MSE\n",
    "        study.optimize(objective, n_trials=20)\n",
    "\n",
    "        # Instantiate the best model based on the Optuna results\n",
    "        best_model_name = study.best_trial.params[\"model_name\"]\n",
    "        if best_model_name == \"Random Forest\":\n",
    "            best_model = RandomForestRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"Decision Tree\":\n",
    "            best_model = DecisionTreeRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"Gradient Boosting\":\n",
    "            best_model = GradientBoostingRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"Linear Regression\":\n",
    "            best_model = LinearRegression()\n",
    "        elif best_model_name == \"XGBRegressor\":\n",
    "            best_model = XGBRegressor(**study.best_trial.params)\n",
    "        elif best_model_name == \"CatBoosting Regressor\":\n",
    "            best_model = CatBoostRegressor(**study.best_trial.params, verbose=False)\n",
    "        elif best_model_name == \"AdaBoost Regressor\":\n",
    "            best_model = AdaBoostRegressor(**study.best_trial.params)\n",
    "\n",
    "        # Train the best model on the entire training set\n",
    "        best_model.fit(X_train, y_train)\n",
    "        logging.info('Find best model completed')\n",
    "        return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DS_project\\ML_project\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "d:\\DS_project\\ML_project\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file_path = r'D:\\DS_project\\ML_project\\src\\notebook\\data\\student.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "num_features = df.select_dtypes(exclude='object').columns\n",
    "df_model = df.copy()\n",
    "df_model['average_score'] = df[num_features].sum(axis=1)/len(num_features)\n",
    "select_columns = ['math_score','reading_score','writing_score']\n",
    "df_model.drop(select_columns,axis=1,inplace=True)\n",
    "X = df_model.drop(columns=['average_score'],axis = 1)\n",
    "y = df_model['average_score']\n",
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder(sparse=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "         (\"StandardScaler\", numeric_transformer, num_features),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "# Fit the OneHotEncoder\n",
    "oh_transformer.fit(X[cat_features])\n",
    "\n",
    "# Get column names for one-hot encoded features\n",
    "encoded_column_names = oh_transformer.get_feature_names_out(cat_features)\n",
    "\n",
    "# Combine column names of one-hot encoded and numerical features\n",
    "final_column_names = list(encoded_column_names) + list(X.select_dtypes(exclude=\"object\").columns)\n",
    "\n",
    "# Convert the transformed array to a DataFrame with column names\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=final_column_names)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded_df,y,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 17:41:05,701] A new study created in memory with name: no-name-dbd0374b-cea0-471c-8277-685e149ab0bf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-08 17:41:06,353] Trial 0 finished with value: 198.71163994593883 and parameters: {'model_name': 'Random Forest', 'rf_n_estimators': 78}. Best is trial 0 with value: 198.71163994593883.\n",
      "[I 2024-03-08 17:41:06,368] Trial 1 finished with value: 153.96108778211806 and parameters: {'model_name': 'Linear Regression'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:06,516] Trial 2 finished with value: 166.5269935091893 and parameters: {'model_name': 'Gradient Boosting', 'gb_learning_rate': 0.019743712509384153, 'gb_n_estimators': 49, 'gb_subsample': 0.886410972618038}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:07,755] Trial 3 finished with value: 160.61830029197125 and parameters: {'model_name': 'AdaBoost Regressor', 'ab_learning_rate': 0.01616286535755046, 'ab_n_estimators': 212}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:08,298] Trial 4 finished with value: 157.83056457049918 and parameters: {'model_name': 'Gradient Boosting', 'gb_learning_rate': 0.016938908612781314, 'gb_n_estimators': 214, 'gb_subsample': 0.6496895701514017}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:10,474] Trial 5 finished with value: 217.4590783257404 and parameters: {'model_name': 'Decision Tree', 'dt_splitter': 'random'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:10,792] Trial 6 finished with value: 159.72585903389427 and parameters: {'model_name': 'Gradient Boosting', 'gb_learning_rate': 0.04042548929801348, 'gb_n_estimators': 121, 'gb_subsample': 0.7418569471663461}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:12,644] Trial 7 finished with value: 198.0371440734785 and parameters: {'model_name': 'Random Forest', 'rf_n_estimators': 234}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:12,660] Trial 8 finished with value: 153.96108778211806 and parameters: {'model_name': 'Linear Regression'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:12,677] Trial 9 finished with value: 153.96108778211806 and parameters: {'model_name': 'Linear Regression'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:13,091] Trial 10 finished with value: 181.34389617524897 and parameters: {'model_name': 'XGBRegressor', 'xgb_learning_rate': 0.017261872209834703, 'xgb_n_estimators': 183}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:13,111] Trial 11 finished with value: 153.96108778211806 and parameters: {'model_name': 'Linear Regression'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:14,247] Trial 12 finished with value: 160.79071720172223 and parameters: {'model_name': 'CatBoosting Regressor', 'cb_depth': 6, 'cb_learning_rate': 0.06228183071420812, 'cb_iterations': 32}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:14,263] Trial 13 finished with value: 153.96108778211806 and parameters: {'model_name': 'Linear Regression'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:14,281] Trial 14 finished with value: 153.96108778211806 and parameters: {'model_name': 'Linear Regression'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:14,297] Trial 15 finished with value: 153.96108778211806 and parameters: {'model_name': 'Linear Regression'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:14,394] Trial 16 finished with value: 163.57010793101188 and parameters: {'model_name': 'AdaBoost Regressor', 'ab_learning_rate': 0.09892676005894785, 'ab_n_estimators': 13}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:14,479] Trial 17 finished with value: 171.95416677254647 and parameters: {'model_name': 'XGBRegressor', 'xgb_learning_rate': 0.09701174464418491, 'xgb_n_estimators': 16}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:14,505] Trial 18 finished with value: 220.03929981651262 and parameters: {'model_name': 'Decision Tree', 'dt_splitter': 'best'}. Best is trial 1 with value: 153.96108778211806.\n",
      "[I 2024-03-08 17:41:16,066] Trial 19 finished with value: 167.56580167158668 and parameters: {'model_name': 'CatBoosting Regressor', 'cb_depth': 10, 'cb_learning_rate': 0.012056647869621864, 'cb_iterations': 98}. Best is trial 1 with value: 153.96108778211806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n"
     ]
    }
   ],
   "source": [
    "find_model = ModelTrainer()\n",
    "get_model = find_model.initiate_model_train(X_train, X_test, y_train, y_test)\n",
    "print(get_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
